\chapter{SaRoMaN simulation and software}
%\chapter{Software development}
\label{c:software}

\section{Introduction}

The software environment used for Baby MIND is the Simulation and Reconstruction of Muons and Neutrinos (SaRoMaN) software suite which is a comprehensive software for MIND/nuSTORM detectors and has been developed at the University of Glasgow over several iterations~\cite{27Bross,  53Laing, 54NUFACT2016Hallsjo}. The software has been expanded to be able to model and simulate a generic detector with limitations in the current implementation of the reconstruction. It includes a complete range of functionality for simulating single particle beams, through GEANT4~\cite{Geant4} or neutrino beams, through GENIE~\cite{Genie}, including geometry design, digitisation and reconstruction through RecPack~\cite{RecPack}. The software suite is also shipped with several analysis code examples written in ROOT~\cite{Root}. It has been created exploiting software engineering and object-oriented techniques and implemented in the C++ and Python programming languages. The software is accessible on request from \url{https://lspace.ppe.gla.ac.uk}. 

\section{General structure}
The SaRoMaN software's main design goals have been to promote modularity, provide a single point of entry for the design and input variables and to simplify usage. With this in mind, the SaRoMaN software is split into four main parts which can be replaced or altered independently of the others as long as the input/output flow is conserved. The parts are denoted, wrapper, simulation, digitisation and reconstruction with the main flow regulated by the wrapper seen in \FigRef{fig:codeFlow} and a more detailed view in \FigRef{fig:structure}. Each part will be discussed further below.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{figures/codeFlow.png}
\caption{Code flow of the SaRoMaN software suite all controlled and handled through a wrapper.}
\label{fig:codeFlow}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{figures/Software_structure.pdf}
\caption{Code structure of the SaRoMaN software suite all controlled and handled through a wrapper.}
\label{fig:structure}
\end{figure}

%The second simulation etc etc.
%Add diagram of code, structure etc. Perhaps some code and or file structures in the appendix.
%Plot the overlaying idea and structure. Stress modularity, charge, momentum and particle type

\section{Wrapper}
To simplify the installation, compilation and usage of the other parts in the SaRoMaN suite, a wrapper has been developed in the Python programming language. This wrapper, aside from the above, handles all input variables, file names, writing of configuration files, standardisation of magnetic field maps and geometry, and data flow between the other parts. 

To simplify operation for the user, all of the installation, compilation and running are handled through simple command line inputs with the option of more advanced commands being issued through the use of the Python wrapper class. After installation, SaRoMaN can be run with some default parameters, a full run diagram can be seen in \FigRef{fig:block}.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{figures/block.pdf}
\caption{Example of how to run a default simulation.}
\label{fig:block}
\end{figure}

\pagebreak
\section{Simulation}
Simulations are used to test and model how particles will interact with a detector model and what scintillator hits can be expected. The outputs from the simulation include the position of the hit on a bar, the location of a bar, the time of the hit and the amount of energy that was deposited. For the studies performed in SaRoMaN, well tested physics models are used in GEANT4, however the option exists to add new physics and to test new theoretical models.

The simulation comes with two different modes, neutrino or ``single particle''. For neutrino mode, GENIE is first run to generate the neutrino events. The data from these events are run through GEANT4, which includes details of the detector geometry and materials, to create the response of the detector to the particle hits. In the single particle mode, GENIE is not invoked, and one can choose to run with a beam of charged particles of a single species, which are also passed onto GEANT4 that simulates the response of the detector.

%These events are later populated and the detector constructed through GEANT4. In a single particle mode a beam of a single particle type is assumed, GENIE is not called and the simulation and detector construction are handled through GEANT4.

A unique feature is that SaRoMaN uses a single geometry definition for the whole framework, written in the Geometry Description Markup Language (GDML)~\cite{GDML} which is interpreted in GEANT4 and is passed as a simplified .txt file to the reconstruction, which simplifies any changes of the geometry. This GDML file can be used to describe simple detectors, such as a monolithic scintillator module, as well as the full Baby MIND and even any combination of several of these put together in any layout. It is also possible to generate a GDML file from computer-aided design (CAD) software to get all of the constructional details from the detector.

%\pagebreak
\section{Digitisation}
Digitisation is the emulation of the signals expected to be produced by the detector hardware. It needs to handle the response of the electronics and describes the expected output signals, based on input hits in the detector. Any electrical system can be described as providing an output based on the response function of the system and the input signal. The digitisation is based on a description of the response function of the detector for the simulated input.

This is currently done in a simplified way by smearing data with different Poisson or Gaussian distributions to take into account the stochastic nature of the production of the expected signals, as well as handling events which are distinguished by a large time difference. For the Baby MIND detector the algorithms take the horizontal and vertical bar hits and clusters together to construct $x$, $y$ and $z$ positions along with the energy deposition and time to produce physical hit points. The full program flow can be seen in \FigRef{fig:digi}.

As a way to simplify the integration and implementation, the data acquisition (DAQ) used for the different test beams have been implemented in the digitisation as well. In this mode, real data is given as input and only the clustering takes place. Due to the usage of a single geometry, a token simulation has to be run in this mode as well to properly construct the geometry.

To ensure that all further analysis is performed properly, the output of the digitisation is identical, whether it was simulated or read out through the DAQ.
%there is no way to distinguish data coming out of the digitisation as being simulated or read in through the DAQ.

One of the most difficult design features of Baby MIND is combining hits from both vertical and horizontal bars. It is possible to get hits which cannot be combined without ambiguity. Figure~\ref{fig:BarAmbi} contains an illustration of two sets of simultaneous hits in the vertical and horizontal bars. It is not possible to distinguished the green hits from each other and the same with the red hits. The current method is for the digitisation to create both hits at all times and these hits are then handled by the reconstruction where they can be distinguished by looking at the pattern recognition and the track reconstruction.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{figures/Digitisation.jpg}
\caption{Program flow for the digitisation.}
\label{fig:digi}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.5\textwidth]{figures/BarsAmbi.jpg}
\caption{Illustration of the difficulty in combining hits in vertical and horizontal bars when there are two or more hits in each bar.}
\label{fig:BarAmbi}
\end{figure}

\subsection{Simulated data}

The digitisation is provided with a .root file~\cite{Root} containing all of the information from the simulation. Given that the data is assumed to be analysed offline, the first task is to separate any hits into event time windows, where hits can be seen as coming from the same particle. The main difficulty is to allow for the particle to fully traverse the detector and to allow a delay in electronics response within the time window. This provides a number of hits in several time bins, denoted as events, without being able to filter the events by using the position information.
%without having any filter by use the position.

After the timing clustering, so-called 3D space points are created by combining the vertical and horizontal bar hits for a specific module. These 3D points are combined to provide an $x$, $y$ and $z$ coordinate. For each bar, coincidence and overlaps are taken into account, as mentioned previously, by creating all of the possible space points allowed by the combination of hits. A clustering algorithm is used to combine the energy, position and hit time of multiple hits, to create one or several clusters with a discrete position.% based on bar overlap, deposited energy and hit time.

The final step in the digitisation is to smear the value of the deposited energy and hit time based on measured values of propagation time through the scintillator bars, given the simulated hit position and not only the bar position.

\subsection{Data acquisition}
%\textbf{Here or test beam?}

The current data acquisition (DAQ) framework for the Baby MIND detector is based on the event building to be carried out offline,  meaning it is processed both after and away from the acquisition to combine the information from multiple FEBs and to create full events offline.

For simplicity, the DAQ converts the acquired data into the same format as expected from a simulation so that the digitisation can be used in the same way. The main difference is that the smearing is only used for simulated data.

%The main difference is that the smearing used for simulated data is turned off. 

In this simplified version of the data acquisition system written for the Baby MIND test beam, the data is stored in different data files specific for each FEB. In the future, the DAQ shall be adapted to merge the data from each FEB, but this was not possible for the test beams. The data from each FEB has channel number, hit time and hit amplitude. The first step is to convert the data into space-positions, using either the vertical or horizontal bar position information and $z-$position. This is carried out by using a database which correlates the FEB and channel number with the knowledge of the physical layout of the detector. The second step is to ensure that each hit position has a corresponding time and amplitude. Step three is to cluster all of the hits in specific time slots, which are equivalent to the event time windows. When this is achieved, several low level filters are performed, to ensure that an event has enough hits to produce at least four 3D space positions in a time slot. After these steps, the data is equivalent to the data produced by a simulation and is processed in the same manner by the reconstruction.%to be passed onto the reconstruction.

%\pagebreak
\section{Reconstruction}\label{sec:reconstruction}
The reconstruction takes the digitised hit data and assembles tracks and vertices. The main physics parameters are the momentum as well as the charge of the particle. The main problems occur when there are overlapping hits with multiple occupancy per bar. In this situation, it is difficult to distinguish between valid hits and to extract the momentum of the track and its charge.
The first problem is handled by removing and adding hits and using a $\chi^2$ analysis through a Kalman fitter package, RecPack~\cite{RecPack}, to find the best trajectory that fits all hits. The second problem is solved through various algorithms to estimate the charge and momentum using different fits.

For simplicity, the reconstruction currently contains a mode denoted as "test beam" where it does nothing more than writes out the input data into a final .root file.

\subsection{Structure}
The reconstruction software has been structured to ensure that the software is as modular as possible. It is currently based on the Kalman fitter package known as RecPack~\cite{RecPack} to provide both a framework and back bone for handling track objects. It is still used since it was the first Kalman filter implementation, however SaRoMaN would benefit from using a bespoke Kalman fitter implementation to ensure its maintenance and reliability, as RecPack is no longer maintained and its documentation is lacking. 
%It is used for historical reasons, however SaRoMaN would benefit from using a Kalman fitter implementation which is still maintained, development, with few and understood bugs and has documentation.

A Kalman fitter is given an underlying model of the trajectory, in our case knowing that particles travel as helices, and a model of the detector as well as other known inputs to the system to form an estimate of the different possible states of the system in a way that is better than using only a single measurement. This is particularly well suited in particle physics where random noise is given through multiple scattering. It can also take into account the energy loss through different materials and keeps track of the varying magnetic field. The measurements can be used in different parts of the detector to estimate both how the particle has and will continue its propagation. Compared to other models, a Kalman fitter has no underlying assumption about the errors being Gaussian.

In high energy physics one frequently faces the problem of modeling the evolution of a dynamic system from a set of experimental measurements. Most reconstruction programs use similar methods. However, in general they are reimplemented for each specific experimental setup. Some examples are fitting algorithms (i.e. Kalman Filter), equations for propagation, random noise estimation (i.e. multiple scattering), model corrections (i.e. energy loss, inhomogeneous magnetic field, and physics measurements.), model conversion, etc. Similarly, the data structure (measurements, tracks, vertices, etc.), which can be generalised as well, depends on the particular implementation. The motivation for using RecPack is due to the combination of these properties into a simple software package.

SaRoMaN has been split into a number of phases: it first initialises the fitter and then it uses an initial pattern classification to choose reconstruction mode. Currently particles and showers are identified, but only muon-like particles are reconstructed. In this identification, a track is produced along with any off-track hits. Finally, the fitter performs a final charge and momentum estimate on the track and tries to fit more tracks to the off-track hits. If this is a neutrino sample, proton/neutron tracks are only built if it finds an initial muon track.

The test beam mode ignores the event classification and uses the fitter to pass information from input through to the final root file.

The reconstruction has split into several minor tasks.
\begin{itemize}
\item Handling data: Pushing the input data into a final root file.
\item Track building / pattern recognition, given an event, (a time window) can we build a track from this?
\item Fitting, with a candidate track, is it possible to add more hits to the track and make it longer? Is it possible to find secondary tracks? Also, is it possible to estimate the momentum and charge of the track?
\end{itemize}

%How does the code work? When does the split happen? FitTrack in example Event by event. Goes on into fitter. split test beam or normal mode. Test beam only passes on the data with some minor calculations, straight line fit etc. Build planes. Fill planes with hits. After this, into classifier. In classifier, try to build trajectories. Find single "paths/tracks" if not, handle occupancy. Make a plot of this! Have all of the different modes here. Handle low and normal mode. Use or don't use recpack. Chi square analysis, add hits etc etc.  After classifier do a final fitter and write all of the data into a root file. For neutrinos handle short secondary tracks, atleast check for them. In the current implementation all parts use some part of RecPack. Stress modularity. Plot with flow through reconstruction.

\subsection{Pattern recognition}
The pattern recognition tries to estimate if the event is muon-like or not. It starts by looking at the number of hits for a given event. There are currently two main limitations to a track being fitted fitted in SaRoMaN:
\begin{itemize}
\item At least 4 hit modules are required to do any form of charge estimation or track building.
\item At least 10 hit modules are required to perform a Kalman fit.
\end{itemize}
Outside of these limitations, currently only muon track fitting has been implemented. The general assumption is that each event will contain at least one muon track followed by other secondary tracks. If this is not the case the event is discarded. The event filtering is performed very simply by assuming anything that is not a shower is a muon. Single hits here refer to separable space points, meaning that for each $z-$position there is only one unique hit. The aim of the pattern recognition is to find as many tracks with single hits as possible. In the pattern recognition, tracks are built up from so-called track stubs of at least four single hits. As soon as a track stub is created other hits can be added onto the track stub to create a final track using a $\chi^2$analysis as a metric. This $\chi^2$ analysis is run through the fitter and requires an initial charge and momentum guess. This means that the best suitable hits are added to the track stub and the other hits are saved for use in finding secondary tracks. 

The program flow can be seen as taking in an event consisting of a number of hits and returning a number of tracks along with hits which could not be fitted into any track. Each returned track contains only single hits as well as a momentum and charge estimate. The latter two are then improved by the fitter.

%Given an event, a time selection which can be seen as part of a accelerator spill cycle. Is it possible from the ensemble of hits to find a track or several tracks. Currently makes the assumption that tracks will be separable in space for at least 4 points or that there are 4 planes with single track points as to have an initial track to later add more points into.

\subsection{Fitting}
The fitter's main job is to build tracks and to estimate the momentum and charge of that track. There are two modes, one main mode for hits with more than nine hits are passed into the Kalman fitter RecPack. When it is not possible to perform a helical fit and for any track with more than three hits, a self implemented lever-arm approach is used. In both cases RecPack is used to build the tracks, however the momentum and charge reconstruction has to be split depending on the possibility of a helical fit or not. The full helix equation (equations 4.1-4.3 and 4.4-4.6) has a total of nine parameters and takes ten measurements to be fully fitted.  
\begin{align}
x(t) &= a\cos(bt+c)+d\\
y(t) &= e\sin(ft+g) + h\\
z(t) &= kt
\end{align}

The helix can be related to physical quantities as:
%This can also be written in a more helpful way as.
%http://www-jlc.kek.jp/subg/offl/lib/docs/helix_manip/main.html

\begin{align}
x(\phi) &= x_0 + d_p \cos \phi_0 + \frac{\alpha}{\kappa}(\cos \phi_0 - \cos(\phi_0 + \phi))\\ 
y(\phi) &= y_0 + d_p \sin \phi_0 + \frac{\alpha}{\kappa}(\sin \phi_0 - \cos(\phi_0 + \phi))\\
z(\phi) &= z_0 + d_z - \frac{\alpha}{\kappa} \tan \lambda \phi
\end{align}
where $\vec{X} = (x_0, y_0, z_0)$ is the arbitrary helix pivot point, $d_p$ is the distance from the helix to the pivot point in the $xy$ plane, $\phi_0$ is the azimuthal angle from the pivot point with respect to the helix center, $\kappa$ is the signed reciprocal transverse momentum, $d_z$ is the distance of the helix from the pivot point in the z direction, and $\tan\lambda$ is the dip angle. The deflection angle $\phi$ is measured from the pivot point and specifies the position of the charged particle on the helical track. The variable $\kappa$ can be used to relate to the transverse particle momentum, $p_T$ and to the magnetic field as 
\begin{align}
 \kappa &=  Q/p_T \\  
 \rho &=  \alpha/\kappa
 \end{align}
where Q is the particle charge, $\rho$ being the signed radius of the helix, and $\alpha \equiv 1/c B$ being a magnetic-field-dependent constant with $c$ as a constant and $B$ the magnetic field strength. The full particle momentum can be obtained as: 
\begin{equation}
\vec{p} = - \frac{Q}{\alpha} \frac{d\vec{X}}{d\phi} = \frac{1}{|\kappa |} 
 \begin{pmatrix}
 -\sin(\phi_0 + \phi)\\
 cos(\phi_0 + \phi)\\
 tan\lambda
 \end{pmatrix}.
\end{equation}

In practice, this fit also takes both energy loss in the detector and multiple scattering into account when calculating the $\chi^2$ values used to fit the parameters. This motivates the requirement for two different modes. If a helical fit is possible RecPack performs the fitting and returns a final fitted helix, within provided measurement errors, with a momentum and charge estimate.

If a helical fit is not possible, denoted as the low momentum case, several different algorithms are used.

The fitting is performed using the Kalman filter algorithm, by using this underlying equation and assuming each measurement point has some error. Each measurement point $\vec{X}$ is related to the next through extrapolation of the helical equation and assuming an error: $\vec{X}_{k+1} = \vec{F}_{k+1,k}\vec{X}_k + \vec{w}_k$. It is in this error term where multiple scattering and energy loss is taken into account. The output is then given as: $\vec{Y}_{k} = \vec{H}_{k}\vec{X}_k + \vec{v}_k$. The helical equation goes into forming the matrix $F$ and $H$.

It should be noted that, in Baby MIND, a helix model is an approximation given that there is no fully encompassing magnetic field and various different materials, iron and scintillator, in the detector. A more correct, but much more complex model would be a helix in the iron with straight line segments in between the non-magnetised air gaps and scintillator modules.

\subsection{Low momentum algorithms}
The force applied to a particle travelling through a magnetic field is given by the Lorentz force:
\begin{equation}
\vec{F}=q\frac{\vec{v}\times\vec{B}}{c}
\end{equation}
where $c$ is the speed of light in vacuum, $\vec{v}$ is the velocity, $q$ the charge and $\vec{B}$ the magnetic field vector.

Using Newton's second law one produces the following differential vector equation.
\begin{equation}
\vec{F}=m\vec{a}=m\dot{\vec{v}}=q\frac{\vec{v}\times\vec{B}}{c}
\end{equation}
The solution to this equation is given by:
\begin{equation}
\vec{p}=m\vec{v}=q\frac{\vec{l}\times\vec{B}}{c}
\end{equation} 
where $\vec{p}$ is the momentum of the particle and $\vec{l}=\vec{X}-\vec{X_0}$ is a vector containing the lengths traversed through the magnetic field.

Using the definition of the cross-product, writing this as a scalar equation, using the small angle approximation and writing using S.I. units produces the normally recognisable equation
\begin{equation}
p = 0.3 QB_\bot \frac{|\vec{l}|}{\theta}=0.3 QB_\bot R
\end{equation}
where $p$ is the momentum in MeV/c, the charge $Q$ is in units of $e$, the electron charge, the magnetic field strength $B$ is in Tesla and the radius of curvature $R$ is in m. This is one of the main equations used for momentum reconstruction in SaRoMaN. The charge is then reconstructed using $p/|p|$. 

\begin{figure}[h!]
\centering
\includegraphics[width=0.5\textwidth]{figures/lowP/scattering.jpeg}
\caption{Track bending inside a magnetised iron plate, measured by four scintillator detectors~\cite{117SABA}.}
\label{fig:Scattering}
\end{figure}

A previous study performed in the collaboration showed that the best best way of evaluating the charge using this method comes from comparing both the first and second bending in the detector, seen in~\FigRef{fig:Scattering}, using two different distributions to handle the scattering.

\begin{figure}[h!]
\centering
%\includegraphics[width=0.5\textwidth]{figures/lowP/null.jpg}
\includegraphics[width=0.8\textwidth]{figures/equationFix2.jpg}

\includegraphics[width=0.8\textwidth]{figures/equationFix1.jpg}
\caption{Illustration of the angular distributions and criteria for selecting charge.}
\label{fig:NullHyp}
\end{figure}

Assuming the distribution of multiple scattering angles and the notation in \FigRef{fig:NullHyp}, such as $f(\Delta_1 | \mu )$ representing the distribution of the measured angle $\Delta_1$ given an assumed charge, the choice of charge is given by the ratio of probabilities.

The algorithm returns a $\mu^-$ if 
\begin{equation}
\frac{f_{\mu^-}(\Delta_1)}{f_{\mu^+}(\Delta_1)} > \frac{f_{\mu^+}(\Delta_2)}{f_{\mu^-}(\Delta_2)}
\end{equation}
and it returns a $\mu^+$ if
\begin{equation}
\frac{f_{\mu^+}(\Delta_1)}{f_{\mu^-}(\Delta_1)} > \frac{f_{\mu^-}(\Delta_2)}{f_{\mu^+}(\Delta_2)}
\end{equation}

%link from http://pdg.lbl.gov/2000/passagerpp.pdf
%Also, look at mind_rec/detector.c


The bending angles are calculated from both measurement planes, (or only one if not possible) and tested against a null-hypothesis of having deflection from only multiple scattering~\cite{13PDG} given by:

\begin{equation}
\theta_0 = \frac{13.6 MeV}{\beta cp} z \sqrt{x/X_0}[1+0.038\ln(x/X_0)]
\end{equation}
where $\beta c$ it the velocity, $z$ is the number of elemetary charges of the incident particle and $x/X_0$ is the thickness of the scattering medium in radiation lengths.

The probability, using this notation, is given in equation~\ref{eq:probScatt}:
\begin{equation}
P_{\mu^\pm} = \frac{1}{2\pi} e^{-(\theta\pm\Delta)^2 / 2\theta_0^2}
\label{eq:probScatt}
\end{equation}

Another goal of the reconstruction is to estimate the momentum. Many different methods using the scattering width or fits have been studied to reconstruct this value in a low momentum region, however the current best approach is to use the continuous slowing down approximation (CSDA). CSDA is based on knowledge of the energy loss in the material before a certain measurement in the detector, assuming the particle deposits an average energy in all materials which it passes. The modular nature of Baby MIND and the varying distance between iron and scintillator planes implies that it has a non-uniform material description. The range of a particle in the detector can be used to extract an initial estimate of the momentum of the particle, and the uncertainty associated with this estimate depends on the number of iron plates between specific scintillator modules. For example, if there are three iron modules between scintillators the error on the momentum is estimated to be approximately s $\sqrt{2} \times 90 MeV/c$. Combining information from the CSDA and the angle produces the final momentum and charge estimate.

%This method cannot be used for tracks which pass through the full detector as then the error is impossible to estimate. 

%If the angular estimate does not work due to multiple scattering a final estimate is used by using a quadratic fit for the few hits provided.

At low momentum, when tracks do not penetrate into Baby MIND very much, a quadratic fit for the first four hits can be carried out to estimate the momentum.

\if{0}
Use the angles to and probability to calculate the charge. Momentum currently from range. Seems better.
Probability of angle from multiple scattering, or from magnetic field. 
Momentum comes from the range calculation, see detector layout. Major gaps in monument,  Potentially could and perhaps should use range momentum as long as tracks stop in detector. Problem with steps, helix better.
We have steps in the tracking detectors.
This seems to be the best approach. If this for some reason fails it goes on to a final attempt.
Quadratic fit, given few hits, try to fit to return charge. Use this only for charge. Momentum from range momentum.
\fi

\subsection{Performance}

%\textbf{Add event display! also for testbeam in next chapter}

During development of SaRoMaN two main metrics were used to evaluate performance, track reconstructed efficiency and charge reconstruction efficiency. These metrics are evaluated by determining how many simulated tracks can be reconstructed compared to the total number of simulated tracks and how many of those reconstructed tracks have the correct charge. This is done for muons of both charges and different momentum values.

Performance will be shown for the initially proposed layout of Baby MIND, for which the software algorithms were designed, the layout implemented for the CERN test beam, and a final study based on the layout used for the WAGASCI detector. The main difference between the initial and test beam layout was to remove the initial measurement plane d0, restructuring a few of the blocks and adding gaps between each of the blocks, as seen in \FigRef{fig:oldMIND}. %This was done from a constructional not a physics motivation.

The charge identification is however improved when utilizing a measurement point from a neutrino target, the WAGASCI module upstream of Baby MIND, however this will reduce the lower limit of reconstruction since the muons must traverse the material of the WAGASCI. 

\begin{figure}[h!]
\centering
\includegraphics[width=0.75\textwidth]{figures/oldStudies/oldMIND.png}

\includegraphics[width=0.75\textwidth]{figures/MIND.jpeg}
%\includegraphics[width=0.48\textwidth]{figures/oldStudies/MINDetam.jpeg}
\caption{(Top) Image of the initial Baby MIND layout. (Bottom) The current Baby MIND layout. The main differences consist of removing the first scintillator plane, a different configuration layout in the middle blocks and removing the final scintillator plane.}
\label{fig:oldMIND}
\end{figure}

In \FigRef{fig:oldMIND2} results for the initially proposed geometry can be seen. The plots have been generated by simulating negative and positive muons in the SaRoMaN framework. The results for the slightly changed test beam geometry layout can be seen in  \FigRef{fig:TestBeamMIND2}. 

The main difference between the results are both below 1~$GeV/c$ and in the region above 1~$GeV/c$ and below 2~$GeV/c$. Below 1~$GeV/c$ the charge reconstruction is slightly worse for the test beam geometry, which can be attributed to the removal of the first scintillating plate d0. There is also a slight change in the shape of the reconstruction efficiency above 1~$GeV/c$ and below 2~$GeV/c$, related to the change of the structure of the detector and additions of gaps providing some drift between measurement positions. For both of the layouts the reconstruction efficiency $> 95\%$ above 0.7~$GeV/c$ and its charge identification efficiency is $> 95\%$ above 0.7~$GeV/c$. For the full region both reconstruction and charge identification efficiencies are $> 80\% $. 

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/oldStudies/oldRecEff.png}

\includegraphics[width=0.8\textwidth]{figures/oldStudies/oldChargeID.png}
\caption{Reconstructed efficiency and charge identification efficiency of the initial Baby MIND layout using the SaRoMaN simulation and reconstruction package.}
\label{fig:oldMIND2}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/oldStudies/FullFitted.pdf}

\includegraphics[width=0.8\textwidth]{figures/oldStudies/FullChargeID.pdf}
\caption{Reconstructed efficiency and charge identification efficiency of the Baby MIND layout configured for the CERN test beam, using the SaRoMaN simulation and reconstruction package.}
\label{fig:TestBeamMIND2}
\end{figure}

%\textbf{Perhaps momentum rec plots as well? Look at old material/collaboration meeting 2 with pull and residuals.}
%\textbf{One plot comparing all of these? Can we compare them? Lower momentum due to TASD?}
\if{0}
Give for different layouts. The initial, well worked. Then work down to the test beam layout and perhaps the latest J-parc layout? 
Charge id, fitting id, momentum pulls and so on.
Look at muons/pions. pure beams with and without TASD/WAGASCI. Especially test beam layout and J-Parc.
Add in the new layout studies?
Quite good, add that charge ID is about x for range y and so on.  Could be improved by changing layout, perhaps even tweaking algorithms etc etc but want to keep generic algorithms to handle different designs.
Momentum pulls and comparisons? Large RMS from RecPack and difficult geometry. Room for improvement.
\textbf{Perhaps cite my Uppsala proceedings?} ~\cite{82Uppsala}.
\fi

\subsection{Unpacking of data}

%\textbf{Based on the FEB readoutprotocol v2.6.}

Currently data has to be recorded by the front-end board and handled by a test beam computer system. The full data contained in the spills has to be written to disk and then it can be processed off-line after the data taking.

The format consists of a time slot, with a time-division multiplexing index (TDM), containing a spill with several triggers. Each global trigger (GTRIG) contains all of the hit data from the various channels at all the times inside the trigger. Hit data contains amplitude and time measurements with the channel information.
Each time measurement comes in steps of 2.5 ns so the system can only distinguish hits on a channel if they happen more than 2.5 ns between each other. The triggers come in blocks of 10 $\mu$s resulting in at most 4000 events per channel per trigger. The format can be seen in \FigRef{fig:FEBstructure}.


\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{figures/febstructure.jpeg}
\caption{TDM, GTRIG, spill and data structure for the FEB readout communication.}
\label{fig:FEBstructure}
\end{figure}

%Add data plots perhaps? Not sure what goes here.
%Should I add details about the data format?
%Explain how the unpacking works, the main data structure assumed in the electronics. The main problem with the data structure, how it is not aimed to work online. 
%Read and use documentation from Yordan Ivanov Karadzhov. Also look at what Aleksander/Sascha has done and what is in the code.


\subsection{Particle identification}
Machine learning algorithms are used through the Toolkit for Multivariate Data Analysis with ROOT (TMVA)~\cite{TMVA} for particle identification. The main goal of the particle identification is to identify muon tracks from other background events with high efficiency. There are several filters trying to perform particle identification, however TMVA is needed to further improve this identification. This will be described in chapter~\ref{c:Testbeam}.

%\subsection{Analysis software}
%Start by combining, if needed, several root files. Continue by choosing interesting root branches, with or without filtering/cutting the data.
%add analysis flows. What is a normal data flow? Add parts to appendix, more data etc etc etc.

%Final root file, read in trees, filter out bad events, if any. Compare apples to apples. 

%\pagebreak
\section{Future development}
Based on the main principles of easily shareable code, using the latest versions and simplicity, a further development of SaRoMaN has already commenced by creating a fully new version based on newer versions of the third party software packages such as GEANT4, GENIE and ROOT. By moving to the latest versions and ensuring that they will be easy to maintain the software becomes ``future proof'' which is very important for ensuring that the results are making use of the latest physics software. The new framework, following the Lord of the Rings theme~\cite{79tolkien2012lord}, has been named SAURON (Simulation And Universal Reconstruction Of Neutrino-like events). SAURON also makes sure to only use open source software and has thus moved from the RecPack Kalman filter to use a Runge-Kutta and Kalman open source fitter known as GENFIT~\cite{81Genfit}.
The software environment and code can be found in a docker environment at: \url{https://hub.docker.com/r/patha325/science/}. The code is hosted at : \url{https://github.com/patha325/NewSaRoMaN}.

The simulation is based on a generic GEANT4 simulation, which takes in neutrino data simulated with the GENIE open source code used by many. The reconstruction package  GENFIT~\cite{81Genfit}, used by several different experiments, takes generic points and uses a Runge-Kutta method to fit the points and calculate the track momentum and charge.

In a similar way to SaRoMaN, the running is handled by a very simple python script which ensures that all software runs coherently with each other. The geometry is handled in a GDML file and read through all different parts to provide one single point of entry for the geometry. To ensure that the code can be used by anyone on different machines without a difficult installation process, the full software is provided in a docker container which means that the code can be run on any operating system. 

To be compatible with the DAQ unpacking software developed for Baby MIND, a secondary docker container has been created to handle the specific root version requirement as well as some other minor software requirements which all clash with the SAURON container.

SAURON is currently in a state were it can be seen as a simplified and novel version of SaRoMaN. Bar representation, clustering and pattern recognition is not currently implemented in the reconstruction, these are all handled by using truth information from the simulation or provided as extra information. None of these are seen to have a major impact on the results, but should be implemented in a future version. In a similar way, the DAQ unpacking is not included, instead the data is assumed to have been processed through it before being passed into the reconstruction. There are currently discussions regarding a unification of the DAQ unpacking and SAURON.

\subsection{Containers}
One of the main issues with scientific software is that they are developed by small teams, often not made publicly available and often the knowledge is only spread through scientific papers. This means that there are many great software solutions and packages which are not used by scientific collaborations, or are misused due to a lack of communication. One could argue that code should be easy to read and that the purpose of a piece of software should be easy to understand, however this assumes that all developers follow some form of coding standard and has/takes the time to develop their code properly which is often not the case. The clear solution to this problem would be to spend more time documenting and cleaning up code as well as sharing it publicly. One problem which may arise is the use of specific third party software versions which make installation quite difficult especially in a climate where documentation is not prioritised and where software is constantly in development.

One proposed solution which solves the problem would be to provide a full runnable coding platform such as a virtual machine. The benefits are numerous, however most of these are further improved by moving to a new software technique known as a container.

\subsubsection{Container vs Virtual machine}

A container is similar to a Virtual machine (VM) but without requiring any abstraction of hardware, they also run on the OS kernel not requiring an added layer. In essence, a container includes an application with the code and all its dependencies without also requiring a full copy of an operating system. This reduces the size of a container compared to a VM and decreases boot time.
%n handle more applications and require fewer VMs and Operating systems.

\subsubsection{Container for particle physics simulations}
There exist several containers which can be combined for various particle physics simulations. There is currently an official version of the root software in a container \url{https://root.cern.ch/root-docker-container-alpha-version} as well as various unofficial containers for software such as GEANT4.

During the development of SAURON, early versions of the container provide good tools for running particle physics simulations, without providing the SAURON framework or the neutrino specific parts. There are also early versions which provide the GENIE neutrino generator which provides a good staring point for running neutrino simulations.

\if{0}
\subsection{Machine learning in particle physics}

There has been some development of using machine learning techniques for various physics analysis as well as in event selection or particle identification for events in detectors. Focusing on the neutrino physics field, there have been a lot of interesting articles relating to how to identify neutrino events in Argon detectors using machine learning~\cite{83Radovic2018, 84Adams}. 

%In the field of neutrino physics machine learning is not often used, some plots for dune etc etc. This was used in this collaboration using TMVA for particle identification/particle selections to chose muons in a background sample for the test beam.
%For the reconstruction, kalman filtering and cellular automaton for few hit events, are used as well.
%Outside of this novel software frameworks such as virtual machines, docker containers and continuous integration techniques have been used.

%What is machine learning? How is it used today? Techniques for particle physics etc. novel ML techniques.

%\subsubsection{Machine learning in particle physics}

%\subsubsection{Machine learning applied to event identification}

%Show TMVA plots, comparing normal likelihood techniques to machine learning, different applications and different methods. Explain the different methods.

%Relating more to data science than expected results from physics.

%Already been done, needs improvement.

\subsection{Other good practises}

\subsubsection{Versioning}
Currently the software is provided as is without any main release or naming convention. It would be beneficial to adapt a good versioning convention by tagging the software on gitlab. The container used has been provided with version numbers in an adhoc manor which should be tied into the software version number.

\subsubsection{Continuous integration}
For the SaRoMaN software a continuous integration platform was provided to ensure that installation of third party software as well as simple test analysis would always work regardless of bad committed code or broken download links. Something similar has been started for SAURON but not implemented fully and it is clear that it could be extended with thorough analysis and more specific test examples.

\subsubsection{Documenting}
Due to the time constraint no documentation exists for either of the software frameworks. The direct recommendation would be to spend time developing some doxygen documentation for the SAURON framework along with an easy how-to guide for new users.
\fi
\section{Summary}
In this chapter the software framework for Baby MIND has been described, as well as future ongoing developments for improving the software framework. The reconstruction and charge identification efficiencies were evaluated for a series of Baby MIND detector configurations, using the SaRoMaN simulation and reconstruction software, showing that it can deliver high efficiencies over a large momentum range from sub-GeV/c to 10 GeV/c muon momenta.
%There has been a short discussion detailing the some of the metrics used for evaluation and discussion on how to continue the software development.
